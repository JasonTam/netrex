{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from netrex.netrex import FactorizationModel, SequenceModel, generate_sequences\n",
    "from netrex.evaluation import auc_score, mrr_score\n",
    "\n",
    "from netrex.utils import _cpu, _gpu, _minibatch  # deal with it\n",
    "from netrex.netrex2 import ShitNet\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from operator import add, mul\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "embedding_dim = 10\n",
    "minibatch_size = 2048\n",
    "n_iter = 10\n",
    "cuda = False\n",
    "\n",
    "PATH_INTERACTIONS = '../data/snap_amazon/video_games/ratings_unary.msg'\n",
    "PATH_USER_FEATS = '../data/snap_amazon/video_games/user_feats.msg'\n",
    "PATH_ITEM_FEATS = '../data/snap_amazon/video_games/item_feats.msg'\n",
    "PATH_ITEM_IMG_EMBS = '../data/snap_amazon/video_games/img_embs.msg'\n",
    "\n",
    "user_col = 'user_id'\n",
    "item_col = 'asin'\n",
    "ts_col = 'timestamp'\n",
    "\n",
    "context_cols = ['month', 'dayofweek']\n",
    "\n",
    "SEED = 322\n",
    "\n",
    "split_date = '2014-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and some more Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xns_df = pd.read_msgpack(PATH_INTERACTIONS).sample(100000, random_state=SEED)\n",
    "user_feats_df = pd.read_msgpack(PATH_USER_FEATS)\n",
    "item_feats_df = pd.read_msgpack(PATH_ITEM_FEATS)\n",
    "item_imgs_df = pd.read_msgpack(PATH_ITEM_IMG_EMBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dummy context features\n",
    "xns_df['month'] = xns_df['timestamp'].dt.month\n",
    "xns_df['dayofweek'] = xns_df['timestamp'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81513, 5)\n",
      "(18487, 5)\n"
     ]
    }
   ],
   "source": [
    "# Train-Val split\n",
    "# in reality, user_feats_df and item_feats_df should have new categories removed too\n",
    "train_df = xns_df.loc[xns_df[ts_col] < split_date].copy()\n",
    "val_df = xns_df.loc[xns_df[ts_col] >= split_date].copy()\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical dtype\n",
    "for col in [user_col, item_col] + context_cols:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    val_df[col] = val_df[col].astype('category', categories=train_df[col].cat.categories)\n",
    "    \n",
    "cat_d = {\n",
    "    col: pd.CategoricalIndex(train_df[col].cat.categories, name=col)\n",
    "    for col in [user_col, item_col] + context_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    17826\n",
       "True       661\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(val_df[user_col].notnull() & val_df[item_col].notnull()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add User and Item Feats to catalog\n",
    "for col in user_feats_df.columns:\n",
    "    cat_d[col] = pd.CategoricalIndex(user_feats_df[col].cat.categories, name=col)\n",
    "for col in item_feats_df.columns:\n",
    "    cat_d[col] = pd.CategoricalIndex(item_feats_df[col].cat.categories, name=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book-keeping methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(feature_fields_ids: Dict[str, Variable],\n",
    "           feature_maps: Dict[str, nn.Embedding],\n",
    "           col_dep: str) -> Dict[str, Variable]:\n",
    "    \"\"\"\n",
    "    Lookup values with prerequisites in `feature_fields_ids`\n",
    "    These results will eventually find their way in `feature_fields_ids`\n",
    "    \"\"\"\n",
    "    return {\n",
    "        feat_name: feat_map(feature_fields_ids[col_dep]).float().squeeze() if 'img' in feat_name else \n",
    "        feat_map(feature_fields_ids[col_dep]).long().squeeze()\n",
    "        for feat_name, feat_map in feature_maps.items()\n",
    "    }\n",
    "        \n",
    "    \n",
    "def lookups_via_features(feats_df: pd.DataFrame, index_cats) -> Dict[str, nn.Embedding]:\n",
    "    \"\"\"\n",
    "    feats_df: user or item features\n",
    "        index should be user or item id such that the ordering matches user or item codes\n",
    "        each column should be of dtype category\n",
    "    # n_indices: number of users or number of items\n",
    "    index_cats: user or item index catgories (cat_d[user_col].categories) \n",
    "    feature_maps: to be used to convert a user or item id into the corresponding feature code\n",
    "    \"\"\"\n",
    "    n_indices = len(index_cats)\n",
    "    feature_maps = {}\n",
    "    for col in feats_df.columns:\n",
    "        emb = nn.Embedding(n_indices, embedding_dim=1, sparse=False)\n",
    "        emb.weight = nn.Parameter(torch.from_numpy(\n",
    "            feats_df.loc[index_cats][col].cat.codes.values[:, None].astype('float32')))\n",
    "        emb.weight.requires_grad = False\n",
    "        feature_maps[col] = emb\n",
    "    return feature_maps\n",
    "\n",
    "\n",
    "user_feature_maps = lookups_via_features(user_feats_df, cat_d[user_col])\n",
    "item_feature_maps = lookups_via_features(item_feats_df, cat_d[item_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Img Embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EMB_DIMS = item_imgs_df.shape[1]\n",
    "n_indices = len(cat_d[item_col].categories)\n",
    "\n",
    "img_in = nn.Embedding(n_indices, embedding_dim=IMG_EMB_DIMS, sparse=False)\n",
    "img_in.weight = nn.Parameter(torch.from_numpy(\n",
    "    item_imgs_df.loc[cat_d[item_col].categories].fillna(0).values.astype('float32')))\n",
    "img_in.weight.requires_grad = False\n",
    "\n",
    "# img_to_emb = nn.Linear(item_imgs_df.shape[1], embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_feature_maps['img'] = img_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  2.2406   0.0000   0.0000  ...    0.0000   0.6221   0.0000\n",
       "  0.0000   0.0000   0.0000  ...    0.0000   0.0000   0.0000\n",
       "[torch.FloatTensor of size 2x4096]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_feature_maps['img'](Variable(torch.from_numpy(np.array([0,1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fields_ids(user_ids: Variable,\n",
    "                   item_ids: Variable,\n",
    "                   contexts: Variable) -> Dict[str, Variable]:\n",
    "    feature_fields_ids = {\n",
    "        user_col: user_ids,\n",
    "        item_col: item_ids,\n",
    "    }\n",
    "    feature_fields_ids.update(lookup(feature_fields_ids, user_feature_maps, user_col))\n",
    "    feature_fields_ids.update(lookup(feature_fields_ids, item_feature_maps, item_col))\n",
    "    feature_fields_ids.update({col: contexts[:, ii] for ii, col in enumerate(context_cols)})\n",
    "    # Potentially more complex feature dependency graphs\n",
    "    \n",
    "    return feature_fields_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from netrex.layers import ScaledEmbedding, ZeroEmbedding\n",
    "from netrex.utils import _cpu, _gpu, _minibatch  # deal with it\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from operator import add, mul\n",
    "from time import time\n",
    "CatsDict = Dict[str, pd.CategoricalIndex]\n",
    "\n",
    "class FNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, cats_d: CatsDict, embedding_dim: int, sparse=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # TODO: I think the book-keeping can be done within\n",
    "        # just using named modules\n",
    "        self.embeddings_d = {\n",
    "            field: ScaledEmbedding(len(cat_index.categories), embedding_dim,\n",
    "                                   sparse=sparse)\n",
    "            for field, cat_index in cats_d.items()\n",
    "        }\n",
    "        self.biases_d = {\n",
    "            field: ZeroEmbedding(len(cat_index.categories), 1,\n",
    "                                 sparse=sparse)\n",
    "            for field, cat_index in cats_d.items()\n",
    "        }\n",
    "\n",
    "        for field, module in self.embeddings_d.items():\n",
    "            self.add_module('embedding_{}'.format(field), module)\n",
    "        for field, module in self.biases_d.items():\n",
    "            self.add_module('bias_{}'.format(field), module)\n",
    "            \n",
    "        self.add_module('img_to_emb', nn.Linear(IMG_EMB_DIMS, embedding_dim))\n",
    "        self.add_module('img_to_bias', nn.Linear(IMG_EMB_DIMS, 1))\n",
    "\n",
    "    def forward(self, user_ids, item_ids, contexts):\n",
    "\n",
    "        feature_fields_ids = get_fields_ids(user_ids, item_ids, contexts)\n",
    "        \n",
    "        # TODO: case with img should be generalized\n",
    "        \n",
    "        # Iter of batch_size X embedding_dim tensors\n",
    "        embeddings = [self.embeddings_d[fields](ids)\n",
    "                      for fields, ids in feature_fields_ids.items() if fields != 'img']\n",
    "        biases = [self.biases_d[fields](ids)\n",
    "                  for fields, ids in feature_fields_ids.items()  if fields != 'img']\n",
    "        \n",
    "        embeddings.append(self.img_to_emb(feature_fields_ids['img']))\n",
    "        biases.append(self.img_to_bias(feature_fields_ids['img']))\n",
    "        \n",
    "\n",
    "        contrib_dot = reduce(add, (\n",
    "            mul(*pair).sum(1)\n",
    "            for pair in itertools.combinations(embeddings, 2))\n",
    "                             )\n",
    "        contrib_bias = reduce(add, biases)\n",
    "\n",
    "        return contrib_dot + contrib_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FModel(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 col_cat_index_d,\n",
    "                 loss='bpr',\n",
    "                 embedding_dim=64,\n",
    "                 n_iter=1,\n",
    "                 batch_size=1024,\n",
    "                 l2=0.0,\n",
    "                 use_cuda=False,\n",
    "                 sparse=False):\n",
    "\n",
    "        self.col_cat_index_d = col_cat_index_d\n",
    "        \n",
    "        self._loss = loss\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._n_iter = n_iter\n",
    "        self._batch_size = batch_size\n",
    "        self._l2 = l2\n",
    "        self._use_cuda = use_cuda\n",
    "        self._sparse = sparse\n",
    "\n",
    "        self._num_users = None\n",
    "        self._num_items = None\n",
    "#         self._net = None\n",
    "        self._net = _gpu(\n",
    "            FNet(self.col_cat_index_d, self._embedding_dim),\n",
    "            self._use_cuda\n",
    "        )\n",
    "\n",
    "\n",
    "    def fit(self, interactions_df, verbose=False):\n",
    "        self._num_users, self._num_items = (len(interactions_df[col].cat.categories)\n",
    "                                            for col in [user_col, item_col])\n",
    "        \n",
    "        interactions = sp.csr_matrix(\n",
    "            (np.ones(len(interactions_df)),\n",
    "             (interactions_df[user_col].cat.codes,\n",
    "              interactions_df[item_col].cat.codes)),\n",
    "            shape=(self._num_users, self._num_items), dtype=np.float32)\n",
    "        \n",
    "        \n",
    "\n",
    "        if self._sparse:\n",
    "            optimizer = optim.Adagrad(self._net.parameters(),\n",
    "                                      weight_decay=self._l2)\n",
    "        else:\n",
    "            optimizer = optim.Adam(self._net.parameters(),\n",
    "                                   weight_decay=self._l2)\n",
    "\n",
    "#         loss_fnc = self._bpr_loss\n",
    "        \n",
    "        for epoch_num in range(self._n_iter):\n",
    "            # todo: shuffle in a more efficient way?\n",
    "            interactions_df = interactions_df.sample(frac=1)\n",
    "            # Storing in tensors\n",
    "            user_ids_tensor = _gpu(torch.from_numpy(interactions_df[user_col].cat.codes.values.astype('int64')),\n",
    "                                   cuda)\n",
    "            item_ids_tensor = _gpu(torch.from_numpy(interactions_df[item_col].cat.codes.values.astype('int64')),\n",
    "                                   cuda)\n",
    "            context_codes_df = pd.concat([interactions_df[col].cat.codes for col in context_cols], axis=1)\n",
    "            context_codes_df.columns = context_cols\n",
    "\n",
    "            context_tensor = _gpu(torch.from_numpy(context_codes_df.values.astype('int64')),\n",
    "                                  cuda)\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            ii = 0\n",
    "            tic = time()\n",
    "            for (batch_user, batch_item, batch_context) in zip(\n",
    "                _minibatch(user_ids_tensor, self._batch_size),\n",
    "                _minibatch(item_ids_tensor, self._batch_size),\n",
    "                _minibatch(context_tensor, self._batch_size)):\n",
    "\n",
    "                user_var = Variable(batch_user)\n",
    "                pos_item_var = Variable(batch_item)\n",
    "                context_var = Variable(batch_context)\n",
    "\n",
    "                pos_score = self._net(user_var, pos_item_var, context_var)\n",
    "                \n",
    "                # TODO: there is some repeat work in `neg_feature_fields_ids`\n",
    "                # that was already done in `pos_feature_fields_ids`\n",
    "                neg_item_var = Variable(_gpu(\n",
    "                    torch.from_numpy(np.random.randint(0,\n",
    "                                                       self._num_items,\n",
    "                                                       len(pos_item_var))),\n",
    "                    self._use_cuda))\n",
    "                neg_score = self._net(user_var, neg_item_var, context_var)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss = (1.0 - F.sigmoid(pos_score -neg_score)).mean()\n",
    "                epoch_loss += loss.data[0]\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                ii += 1\n",
    "                if verbose and (ii%50)==0:\n",
    "                    print('{}: {} \\t {}'.format(ii, time()-tic, loss.mean().data[0]))\n",
    "\n",
    "            if verbose:\n",
    "                print('Epoch {}: loss {}'.format(epoch_num, epoch_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = FModel(cat_d, embedding_dim=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_in_train_df = val_df.loc[val_df[user_col].notnull() & val_df[item_col].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit 1 epoch\n",
    "# model.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How can we eval?\n",
    "# eval will be slow because we cannot groupby user\n",
    "# since we are considering context... oh well\n",
    "# Let's use a ghetto mean avg precision\n",
    "\n",
    "def score_model(model, val_in_train_df, k=100):\n",
    "\n",
    "    user_ids_tensor = _gpu(torch.from_numpy(val_in_train_df[user_col].cat.codes.values.astype('int64')),\n",
    "                           cuda)\n",
    "    item_ids_tensor = _gpu(torch.from_numpy(val_in_train_df[item_col].cat.codes.values.astype('int64')),\n",
    "                           cuda)\n",
    "    context_codes_df = pd.concat([val_in_train_df[col].cat.codes for col in context_cols], axis=1)\n",
    "    context_codes_df.columns = context_cols\n",
    "\n",
    "    context_tensor = _gpu(torch.from_numpy(context_codes_df.values.astype('int64')),\n",
    "                          cuda)\n",
    "\n",
    "    items_all = Variable(\n",
    "        _gpu(torch.arange(0, model._num_items).long(),\n",
    "             model._use_cuda)\n",
    "    )\n",
    "\n",
    "    correct_in_k = []\n",
    "\n",
    "    for (batch_user, batch_item, batch_context) in zip(\n",
    "        _minibatch(user_ids_tensor, 1),\n",
    "        _minibatch(item_ids_tensor, 1),  # Doesn't really need to be tensorized\n",
    "        _minibatch(context_tensor, 1)):\n",
    "\n",
    "\n",
    "        user_rep = Variable(batch_user.repeat(model._num_items))\n",
    "        context_rep = Variable(batch_context.repeat(model._num_items, 1))\n",
    "        preds = model._net(\n",
    "            user_rep,\n",
    "            items_all,\n",
    "            context_rep\n",
    "        )\n",
    "\n",
    "        correct_in_k.append(batch_item.numpy() in np.argpartition(-preds.data.numpy(), kth=k, axis=0)[:k])\n",
    "        gmap = np.mean(correct_in_k)\n",
    "    return gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: 5.2072227001190186 \t 0.33691588044166565\n",
      "Epoch 0: loss 28.170246958732605\n",
      "50: 5.7917163372039795 \t 0.27147749066352844\n",
      "Epoch 0: loss 23.173319905996323\n",
      "50: 5.1279871463775635 \t 0.25476980209350586\n",
      "Epoch 0: loss 20.881321147084236\n",
      "2 0.0257186081694\n",
      "50: 5.141080379486084 \t 0.2327200472354889\n",
      "Epoch 0: loss 19.22147636115551\n",
      "50: 5.16515588760376 \t 0.23553845286369324\n",
      "Epoch 0: loss 18.07867057621479\n",
      "50: 5.158850193023682 \t 0.20227046310901642\n",
      "Epoch 0: loss 17.20361191034317\n",
      "50: 5.168084383010864 \t 0.2085639238357544\n",
      "Epoch 0: loss 16.431586518883705\n",
      "50: 5.087740182876587 \t 0.2276940494775772\n",
      "Epoch 0: loss 16.072343230247498\n",
      "50: 5.078282833099365 \t 0.1743677258491516\n",
      "Epoch 0: loss 15.582215443253517\n",
      "50: 5.12999963760376 \t 0.17785198986530304\n",
      "Epoch 0: loss 15.104512050747871\n",
      "50: 5.25907301902771 \t 0.1822538673877716\n",
      "Epoch 0: loss 14.693519160151482\n",
      "10 0.0302571860817\n",
      "50: 5.393547534942627 \t 0.16423487663269043\n",
      "Epoch 0: loss 14.53006187081337\n",
      "50: 5.259625434875488 \t 0.18090477585792542\n",
      "Epoch 0: loss 14.171174585819244\n",
      "50: 5.3982110023498535 \t 0.15988408029079437\n",
      "Epoch 0: loss 13.579349040985107\n",
      "50: 5.300289630889893 \t 0.17040769755840302\n",
      "Epoch 0: loss 13.298233941197395\n",
      "50: 5.224542617797852 \t 0.1739986538887024\n",
      "Epoch 0: loss 12.97729116678238\n",
      "50: 5.945553779602051 \t 0.16671723127365112\n",
      "Epoch 0: loss 12.659528389573097\n",
      "50: 5.397662401199341 \t 0.15708127617835999\n",
      "Epoch 0: loss 12.317353367805481\n",
      "50: 5.408794164657593 \t 0.1467588096857071\n",
      "Epoch 0: loss 11.918546803295612\n",
      "50: 5.529651641845703 \t 0.1458531618118286\n",
      "Epoch 0: loss 11.626987144351006\n",
      "50: 5.667392730712891 \t 0.15258894860744476\n",
      "Epoch 0: loss 11.423325754702091\n",
      "20 0.0242057488654\n",
      "50: 5.1573545932769775 \t 0.13651500642299652\n",
      "Epoch 0: loss 11.055423997342587\n",
      "50: 5.2922258377075195 \t 0.12445112317800522\n",
      "Epoch 0: loss 10.607571966946125\n",
      "50: 5.456930160522461 \t 0.13558809459209442\n",
      "Epoch 0: loss 10.319133765995502\n",
      "50: 5.193885326385498 \t 0.11993023753166199\n",
      "Epoch 0: loss 9.87153486162424\n",
      "50: 5.2821643352508545 \t 0.14021703600883484\n",
      "Epoch 0: loss 9.644401222467422\n",
      "50: 5.702251434326172 \t 0.11036718636751175\n",
      "Epoch 0: loss 9.263293117284775\n",
      "50: 5.189695358276367 \t 0.11935601383447647\n",
      "Epoch 0: loss 9.054964117705822\n",
      "50: 4.650600910186768 \t 0.12198728322982788\n",
      "Epoch 0: loss 8.628372021019459\n",
      "50: 4.728495836257935 \t 0.10327182710170746\n",
      "Epoch 0: loss 8.414665505290031\n",
      "CPU times: user 26min 31s, sys: 51.3 s, total: 27min 23s\n",
      "Wall time: 11min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = FModel(cat_d, embedding_dim=10, batch_size=1024)\n",
    "scores = []\n",
    "for ep in range(30):\n",
    "    model.fit(train_df, verbose=True)\n",
    "    if ep in {2, 10, 20, 30}:\n",
    "        score = score_model(model, val_in_train_df, k=10)\n",
    "        print(ep, score)\n",
    "        scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
